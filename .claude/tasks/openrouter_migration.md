# Plan de d√©veloppement : Migration OpenRouter pour le chunking

## üìã P√©rim√®tre

**Objectif** : Remplacer OpenAI par OpenRouter (Gemini 2.5 Flash) pour le recodage de texte lors du chunking.

**Ce qui change** :
- ‚úÖ Recodage de texte : `gpt-4o-mini` ‚Üí `openai/gemini-2.5-flash` via OpenRouter
- ‚ùå Embeddings : Garder OpenAI `text-embedding-3-large` (inchang√©)
- ‚ùå OCR : Garder Mistral (priorit√©) / OpenAI (fallback) (inchang√©)

**Comportement attendu** :
1. L'UI affiche un champ texte pr√©-rempli avec `openai/gemini-2.5-flash` dans la section 3.1
2. L'utilisateur peut changer le mod√®le (champ texte libre)
3. Le syst√®me essaie OpenRouter en premier
4. Si OpenRouter √©choue (quelque raison que ce soit) ‚Üí afficher modal de confirmation
5. Si l'utilisateur confirme ‚Üí utiliser OpenAI `gpt-4o-mini` en fallback
6. Si l'utilisateur annule ‚Üí arr√™ter le processus

---

## üîß Modifications techniques d√©taill√©es

### 1. Configuration & Variables d'environnement

#### Fichier : `.env` (racine du projet)

Ajouter les variables suivantes :

```bash
# OpenRouter Configuration
OPENROUTER_API_KEY=sk-or-v1-xxxxxxxxxxxxx
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_DEFAULT_MODEL=openai/gemini-2.5-flash
```

**Notes** :
- `OPENROUTER_API_KEY` : Obligatoire pour utiliser OpenRouter
- `OPENROUTER_BASE_URL` : URL de base de l'API OpenRouter
- `OPENROUTER_DEFAULT_MODEL` : Mod√®le par d√©faut (format OpenRouter : `provider/model-name`)

---

### 2. Backend : scripts/rad_chunk.py

#### 2.1 Ajout du client OpenRouter (apr√®s ligne 70)

**Localisation** : Apr√®s `client = OpenAI(api_key=OPENAI_API_KEY)`

```python
# ----------------------------------------------------------------------
# OpenRouter Client Setup (for chunking with Gemini 2.5 Flash)
# ----------------------------------------------------------------------
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_BASE_URL = os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
OPENROUTER_DEFAULT_MODEL = os.getenv("OPENROUTER_DEFAULT_MODEL", "openai/gemini-2.5-flash")

client_openrouter = None
if OPENROUTER_API_KEY:
    client_openrouter = OpenAI(
        api_key=OPENROUTER_API_KEY,
        base_url=OPENROUTER_BASE_URL
    )
    print(f"‚úì Client OpenRouter initialis√© (mod√®le par d√©faut : {OPENROUTER_DEFAULT_MODEL})")
else:
    print("‚ö† OPENROUTER_API_KEY non trouv√©e, fallback direct sur OpenAI")
```

---

#### 2.2 Nouvelle fonction `gpt_recode_batch_with_openrouter()`

**Localisation** : Ins√©rer juste AVANT la fonction `gpt_recode_batch()` existante (ligne 109)

```python
def gpt_recode_batch_with_openrouter(
    texts,
    instructions,
    model=None,
    temperature=0.3,
    max_tokens=8000
):
    """
    Recode un lot de textes via OpenRouter.
    Retourne (success: bool, cleaned_texts: list, error: str|None)
    """
    if not client_openrouter:
        return (False, None, "Client OpenRouter non initialis√© (cl√© API manquante)")

    if model is None:
        model = OPENROUTER_DEFAULT_MODEL

    cleaned_texts = []
    failed_indices = []

    print(f"  ‚Üí Tentative de recodage via OpenRouter ({model})...")

    try:
        # Traitement en parall√®le avec ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=DEFAULT_MAX_WORKERS) as executor:
            def call_openrouter(text_item):
                text, idx = text_item
                try:
                    response = client_openrouter.chat.completions.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": instructions},
                            {"role": "user", "content": text}
                        ],
                        temperature=temperature,
                        max_tokens=max_tokens,
                        extra_headers={
                            "HTTP-Referer": "https://ragpy.local",  # Requis par OpenRouter
                            "X-Title": "RAGpy Chunking Pipeline"     # Optionnel mais recommand√©
                        }
                    )
                    return (idx, response.choices[0].message.content.strip())
                except Exception as e:
                    print(f"    Erreur OpenRouter pour chunk {idx}: {e}")
                    return (idx, None)

            # Appels parall√®les
            results = list(executor.map(call_openrouter, enumerate(texts)))

        # Collecte des r√©sultats
        for idx, cleaned in results:
            if cleaned is None:
                failed_indices.append(idx)
                cleaned_texts.append(texts[idx])  # Texte original si √©chec
            else:
                cleaned_texts.append(cleaned)

        # Retry s√©quentiel pour les √©checs
        if failed_indices:
            print(f"  ‚Üí Retry s√©quentiel pour {len(failed_indices)} chunks √©chou√©s...")
            for idx in failed_indices:
                try:
                    response = client_openrouter.chat.completions.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": instructions},
                            {"role": "user", "content": texts[idx]}
                        ],
                        temperature=temperature,
                        max_tokens=max_tokens,
                        extra_headers={
                            "HTTP-Referer": "https://ragpy.local",
                            "X-Title": "RAGpy Chunking Pipeline"
                        }
                    )
                    cleaned_texts[idx] = response.choices[0].message.content.strip()
                except Exception as e_retry:
                    print(f"    √âchec d√©finitif pour chunk {idx}: {e_retry}")
                    # Garde le texte original

        print(f"  ‚úì Recodage OpenRouter r√©ussi ({len(cleaned_texts)} chunks)")
        return (True, cleaned_texts, None)

    except Exception as e:
        error_msg = str(e)
        print(f"  ‚úó Erreur globale OpenRouter: {error_msg}")
        return (False, None, error_msg)
```

---

#### 2.3 Ajout d'un argument `--model` au parser CLI

**Localisation** : Modifier la fonction `main()` (rechercher `argparse.ArgumentParser`)

```python
# Dans la section argparse (g√©n√©ralement vers la fin du fichier)
parser.add_argument(
    '--model',
    type=str,
    default=None,
    help='Mod√®le pour le recodage (format OpenRouter: provider/model-name). '
         'Par d√©faut: utilise OPENROUTER_DEFAULT_MODEL si disponible, sinon gpt-4o-mini'
)
```

---

#### 2.4 Modifier `process_document_chunks()` pour utiliser OpenRouter

**Localisation** : Ligne 223, remplacer l'appel √† `gpt_recode_batch()`

**Avant** :
```python
cleaned_batch = gpt_recode_batch(
    batch_to_recode,
    instructions="ce chunk est issu d'un ocr brut...",
    model="gpt-4o-mini",
    temperature=0.3,
    max_tokens=8000
)
```

**Apr√®s** :
```python
# R√©cup√©rer le mod√®le depuis les arguments CLI
model_to_use = getattr(process_document_chunks, '_model_override', None)

# Essayer OpenRouter en premier si disponible
if client_openrouter and model_to_use:
    success, cleaned_batch, error = gpt_recode_batch_with_openrouter(
        batch_to_recode,
        instructions="ce chunk est issu d'un ocr brut qui laisse beaucoup de blocs de texte inutiles comme des titres de pages, des numeros, etc. Nettoie ce chunk pour en faire un texte propre qui commence par une phrase compl√®te et se termine par un point. Supprime le bruit d'OCR et les imperfections en conservant le sens original. Ne echange ni ajoute aucun mot du texte d'origine. C'est une correction et un nettoyage de texte (suppression des erreurs) pas une r√©√©criture",
        model=model_to_use,
        temperature=0.3,
        max_tokens=8000
    )

    if not success:
        # Signal d'√©chec OpenRouter ‚Üí n√©cessite fallback
        raise Exception(f"OPENROUTER_UNAVAILABLE: {error}")
else:
    # Fallback OpenAI (comportement original)
    print("  ‚Üí Utilisation d'OpenAI (gpt-4o-mini) pour le recodage...")
    cleaned_batch = gpt_recode_batch(
        batch_to_recode,
        instructions="ce chunk est issu d'un ocr brut qui laisse beaucoup de blocs de texte inutiles comme des titres de pages, des numeros, etc. Nettoie ce chunk pour en faire un texte propre qui commence par une phrase compl√®te et se termine par un point. Supprime le bruit d'OCR et les imperfections en conservant le sens original. Ne echange ni ajoute aucun mot du texte d'origine. C'est une correction et un nettoyage de texte (suppression des erreurs) pas une r√©√©criture",
        model="gpt-4o-mini",
        temperature=0.3,
        max_tokens=8000
    )
```

**Note** : Ajouter au d√©but de la fonction `process_document_chunks()` un moyen de passer le mod√®le :

```python
def process_document_chunks(csv_file, json_file, model=None):
    # Stocker le mod√®le pour usage dans le traitement
    if model:
        process_document_chunks._model_override = model
    # ... reste du code
```

---

#### 2.5 Modifier la fonction `main()` pour passer le mod√®le

**Localisation** : Vers la fin du fichier, dans `if __name__ == "__main__":`

```python
# Dans la phase 'initial'
if args.phase in ["initial", "all"]:
    # ...
    process_document_chunks(
        args.input,
        json_file,
        model=args.model  # ‚Üê Nouveau param√®tre
    )
```

---

### 3. Backend : app/main.py (FastAPI)

#### 3.1 Nouveau endpoint `/check_openrouter`

**Localisation** : Ins√©rer apr√®s l'endpoint `/save_credentials` (apr√®s ligne 526)

```python
@app.post("/check_openrouter")
async def check_openrouter():
    """
    V√©rifie si OpenRouter est accessible et fonctionnel.
    Retourne {available: bool, error: str|None}
    """
    try:
        openrouter_key = os.getenv("OPENROUTER_API_KEY")
        if not openrouter_key:
            return {"available": False, "error": "OPENROUTER_API_KEY non configur√©e"}

        # Test de connexion simple (appel minimal)
        import requests
        response = requests.get(
            "https://openrouter.ai/api/v1/models",
            headers={"Authorization": f"Bearer {openrouter_key}"},
            timeout=5
        )

        if response.status_code == 200:
            return {"available": True, "error": None}
        else:
            return {
                "available": False,
                "error": f"Erreur HTTP {response.status_code}: {response.text[:200]}"
            }

    except Exception as e:
        return {"available": False, "error": str(e)}
```

---

#### 3.2 Modifier `/initial_text_chunking` pour supporter le mod√®le

**Localisation** : Ligne 582-624

**Modifications** :

1. **Ajouter un param√®tre `model`** dans la fonction :

```python
@app.post("/initial_text_chunking")
async def initial_text_chunking(model: str = Form(default="openai/gemini-2.5-flash")):
    """
    G√©n√®re les chunks initiaux avec recodage GPT.
    Param√®tres:
        - model: Mod√®le OpenRouter √† utiliser (format: provider/model-name)
    """
```

2. **Passer le mod√®le au script** :

```python
# Ligne ~600, modifier la commande subprocess
script_args = [
    sys.executable,
    str(script_path),
    "--input", str(csv_path),
    "--output", str(upload_folder),
    "--phase", "initial",
    "--model", model  # ‚Üê Nouveau argument
]
```

3. **G√©rer l'exception OPENROUTER_UNAVAILABLE** :

```python
# Dans le bloc try/except du subprocess (ligne ~605)
try:
    result = subprocess.run(
        script_args,
        cwd=project_root,
        capture_output=True,
        text=True,
        timeout=3600
    )

    # V√©rifier si l'erreur est li√©e √† OpenRouter
    if result.returncode != 0:
        error_output = result.stderr
        if "OPENROUTER_UNAVAILABLE" in error_output:
            # Extraire le message d'erreur
            error_msg = error_output.split("OPENROUTER_UNAVAILABLE:", 1)[1].strip() if ":" in error_output else "Service inaccessible"
            return {
                "fallback_needed": True,
                "error": error_msg,
                "full_error": error_output
            }
        else:
            # Autre erreur
            raise HTTPException(status_code=500, detail=error_output)

    # Succ√®s normal
    return {"success": True, "message": "Chunking initial r√©ussi avec OpenRouter"}

except subprocess.TimeoutExpired:
    raise HTTPException(status_code=500, detail="Timeout lors du chunking (>1h)")
except Exception as e:
    raise HTTPException(status_code=500, detail=str(e))
```

---

#### 3.3 Nouvel endpoint `/initial_text_chunking_fallback`

**Localisation** : Apr√®s `/initial_text_chunking` (apr√®s ligne 624)

```python
@app.post("/initial_text_chunking_fallback")
async def initial_text_chunking_fallback():
    """
    Re-ex√©cute le chunking initial en for√ßant l'utilisation d'OpenAI.
    Appel√© apr√®s confirmation de l'utilisateur en cas d'√©chec OpenRouter.
    """
    try:
        # Chemins identiques √† /initial_text_chunking
        upload_folder = PROJECT_ROOT / "uploads"
        csv_path = upload_folder / "output.csv"

        if not csv_path.exists():
            raise HTTPException(status_code=404, detail="output.csv introuvable")

        # Chemin du script
        script_path = PROJECT_ROOT / "scripts" / "rad_chunk.py"
        if not script_path.exists():
            raise HTTPException(status_code=500, detail=f"Script introuvable: {script_path}")

        # Commande SANS --model (force OpenAI par d√©faut)
        script_args = [
            sys.executable,
            str(script_path),
            "--input", str(csv_path),
            "--output", str(upload_folder),
            "--phase", "initial"
            # ‚Üê PAS de --model, donc rad_chunk.py utilisera OpenAI
        ]

        print(f"[FALLBACK] Ex√©cution du chunking avec OpenAI : {' '.join(script_args)}")

        result = subprocess.run(
            script_args,
            cwd=PROJECT_ROOT,
            capture_output=True,
            text=True,
            timeout=3600
        )

        if result.returncode != 0:
            raise HTTPException(status_code=500, detail=result.stderr)

        return {
            "success": True,
            "message": "Chunking r√©ussi avec OpenAI (fallback)",
            "output": result.stdout
        }

    except subprocess.TimeoutExpired:
        raise HTTPException(status_code=500, detail="Timeout lors du chunking fallback (>1h)")
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

#### 3.4 Modifier `/get_credentials` pour inclure OpenRouter

**Localisation** : Ligne 412-462

**Modification** : Ajouter la cl√© OpenRouter dans la r√©ponse

```python
# Ligne ~453, ajouter dans le return
return {
    "OPENAI_API_KEY": openai_key,
    "PINECONE_API_KEY": pinecone_key,
    "MISTRAL_API_KEY": mistral_key,
    "WEAVIATE_URL": weaviate_url,
    "WEAVIATE_API_KEY": weaviate_key,
    "QDRANT_URL": qdrant_url,
    "QDRANT_API_KEY": qdrant_key,
    "OPENROUTER_API_KEY": os.getenv("OPENROUTER_API_KEY", ""),  # ‚Üê Nouveau
    "OPENROUTER_DEFAULT_MODEL": os.getenv("OPENROUTER_DEFAULT_MODEL", "openai/gemini-2.5-flash")  # ‚Üê Nouveau
}
```

---

#### 3.5 Modifier `/save_credentials` pour sauvegarder OpenRouter

**Localisation** : Ligne 464-526

**Modifications** :

1. **Accepter les nouveaux param√®tres** :

```python
@app.post("/save_credentials")
async def save_credentials(
    OPENAI_API_KEY: str = Form(default=""),
    PINECONE_API_KEY: str = Form(default=""),
    MISTRAL_API_KEY: str = Form(default=""),
    WEAVIATE_URL: str = Form(default=""),
    WEAVIATE_API_KEY: str = Form(default=""),
    QDRANT_URL: str = Form(default=""),
    QDRANT_API_KEY: str = Form(default=""),
    OPENROUTER_API_KEY: str = Form(default=""),  # ‚Üê Nouveau
    OPENROUTER_DEFAULT_MODEL: str = Form(default="openai/gemini-2.5-flash")  # ‚Üê Nouveau
):
```

2. **Ajouter dans la liste des cl√©s √† sauvegarder** :

```python
# Ligne ~500
credential_keys_to_save = [
    "OPENAI_API_KEY",
    "PINECONE_API_KEY",
    "MISTRAL_API_KEY",
    "WEAVIATE_URL",
    "WEAVIATE_API_KEY",
    "QDRANT_URL",
    "QDRANT_API_KEY",
    "OPENROUTER_API_KEY",  # ‚Üê Nouveau
    "OPENROUTER_DEFAULT_MODEL"  # ‚Üê Nouveau
]
```

---

### 4. Frontend : app/templates/index.html

#### 4.1 Ajouter le champ mod√®le dans la section 3.1

**Localisation** : Ligne 149-158, apr√®s le `<p>` d'avertissement, avant le formulaire d'upload

```html
<!-- Step 3.1: Initial Text Chunking -->
<section id="initial-chunk-section" style="display:none;">
  <h2>3.1 Initial Text Chunking</h2>
  <p style="color: black; font-size: 0.9em;">Warning: This process can take a long time depending on your file. Do not close your browser. We recommend processing a maximum of 10 articles or one book at a time.</p>

  <!-- ‚Üì‚Üì‚Üì NOUVEAU CHAMP MOD√àLE ‚Üì‚Üì‚Üì -->
  <div style="margin-bottom: 15px;">
    <label for="chunkingModel" style="display: block; margin-bottom: 5px; font-weight: bold;">
      Mod√®le de recodage :
    </label>
    <input
      type="text"
      id="chunkingModel"
      name="model"
      value="openai/gemini-2.5-flash"
      placeholder="openai/gemini-2.5-flash"
      style="width: 100%; padding: 8px; border: 1px solid #ccc; border-radius: 4px; font-family: monospace;"
    />
    <small style="color: #666; font-size: 0.85em;">
      Format OpenRouter : <code>provider/model-name</code> (ex: openai/gemini-2.5-flash, anthropic/claude-3.5-sonnet)
    </small>
  </div>
  <!-- ‚Üë‚Üë‚Üë FIN NOUVEAU CHAMP ‚Üë‚Üë‚Üë -->

  <form id="initialChunkUploadForm" enctype="multipart/form-data" class="stage-upload-form">
    <input type="file" id="initialChunkUploadInput" name="file" accept=".csv" />
    <button type="submit">Upload output.csv</button>
  </form>
  <button id="runInitialChunkBtn">Generate Chunks</button>
  <div id="initialChunkResult" class="result"></div>
</section>
```

---

#### 4.2 Ajouter le modal de confirmation fallback

**Localisation** : Apr√®s le modal Settings (ligne 122), avant la section `<script>`

```html
<!-- Modal de confirmation fallback OpenRouter -->
<div id="fallbackModal" class="modal" style="display: none;">
  <div class="modal-content" style="max-width: 500px;">
    <h2 style="color: #d32f2f; margin-top: 0;">‚ö†Ô∏è OpenRouter inaccessible</h2>
    <p id="fallbackErrorMessage" style="margin: 15px 0; padding: 10px; background: #fff3cd; border-left: 4px solid #ffc107; color: #856404;">
      <!-- Le message d'erreur sera inject√© ici par JavaScript -->
    </p>
    <p style="margin: 15px 0;">
      Voulez-vous utiliser <strong>OpenAI (gpt-4o-mini)</strong> √† la place ?
    </p>
    <div style="display: flex; gap: 10px; justify-content: flex-end; margin-top: 20px;">
      <button id="cancelFallback" style="background: #6c757d; color: white; padding: 10px 20px; border: none; border-radius: 4px; cursor: pointer;">
        Annuler
      </button>
      <button id="confirmFallback" style="background: #1976d2; color: white; padding: 10px 20px; border: none; border-radius: 4px; cursor: pointer;">
        Oui, utiliser OpenAI
      </button>
    </div>
  </div>
</div>

<!-- Style du modal -->
<style>
.modal {
  position: fixed;
  z-index: 9999;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  overflow: auto;
  background-color: rgba(0,0,0,0.5);
  display: flex;
  align-items: center;
  justify-content: center;
}

.modal-content {
  background-color: #fefefe;
  padding: 20px;
  border: 1px solid #888;
  border-radius: 8px;
  box-shadow: 0 4px 6px rgba(0,0,0,0.1);
}
</style>
```

---

#### 4.3 Ajouter OpenRouter dans le modal Settings

**Localisation** : Ligne 87-122, dans le formulaire Settings

**Ajouter apr√®s la section OpenAI** (ligne ~95) :

```html
<!-- Section OpenAI existante -->
<h3 style="margin-top:0; margin-bottom: 10px; color: #1976d2;">OpenAI</h3>
<label for="openaiKey">API Key</label>
<input type="text" id="openaiKey" name="OPENAI_API_KEY" autocomplete="off" placeholder="sk-..." />

<!-- ‚Üì‚Üì‚Üì NOUVELLE SECTION OPENROUTER ‚Üì‚Üì‚Üì -->
<h3 style="margin-top: 20px; margin-bottom: 10px; color: #1976d2;">OpenRouter</h3>
<label for="openrouterKey">API Key</label>
<input type="text" id="openrouterKey" name="OPENROUTER_API_KEY" autocomplete="off" placeholder="sk-or-v1-..." />

<label for="openrouterModel" style="margin-top: 10px;">Mod√®le par d√©faut</label>
<input
  type="text"
  id="openrouterModel"
  name="OPENROUTER_DEFAULT_MODEL"
  autocomplete="off"
  placeholder="openai/gemini-2.5-flash"
  style="font-family: monospace;"
/>
<small style="color: #666; display: block; margin-top: 5px;">
  Format : provider/model-name
</small>
<!-- ‚Üë‚Üë‚Üë FIN NOUVELLE SECTION ‚Üë‚Üë‚Üë -->

<!-- Section Pinecone (existante) -->
<h3 style="margin-top: 20px; margin-bottom: 10px; color: #1976d2;">Pinecone</h3>
<!-- ... reste du code ... -->
```

---

#### 4.4 JavaScript : Charger les credentials OpenRouter

**Localisation** : Fonction `loadCredentials()` (ligne 224-269)

**Modification** : Ajouter le chargement des cl√©s OpenRouter

```javascript
// Ligne ~247, apr√®s le chargement de openaiKey
if (data.OPENROUTER_API_KEY) {
  const maskedOR = data.OPENROUTER_API_KEY.substring(0, 20) + '######';
  document.getElementById('openrouterKey').value = maskedOR;
  document.getElementById('openrouterKey').dataset.fullKey = data.OPENROUTER_API_KEY;
}

if (data.OPENROUTER_DEFAULT_MODEL) {
  document.getElementById('openrouterModel').value = data.OPENROUTER_DEFAULT_MODEL;
}
```

---

#### 4.5 JavaScript : Sauvegarder les credentials OpenRouter

**Localisation** : Fonction de soumission du formulaire Settings (ligne 285-322)

**Modification** : Inclure les champs OpenRouter dans FormData

```javascript
// Ligne ~308, avant le fetch
const openrouterKeyInput = document.getElementById('openrouterKey');
let openrouterKey = openrouterKeyInput.value;

// Si la cl√© affich√©e est masqu√©e, utiliser la cl√© compl√®te stock√©e
if (openrouterKey.includes('######') && openrouterKeyInput.dataset.fullKey) {
  openrouterKey = openrouterKeyInput.dataset.fullKey;
}

const openrouterModel = document.getElementById('openrouterModel').value;

// Ajouter √† FormData
formData.append('OPENROUTER_API_KEY', openrouterKey);
formData.append('OPENROUTER_DEFAULT_MODEL', openrouterModel);
```

---

#### 4.6 JavaScript : Gestion du bouton "Generate Chunks"

**Localisation** : Event listener `#runInitialChunkBtn` (chercher dans le code, g√©n√©ralement apr√®s ligne 450)

**Remplacer l'ancien code** par :

```javascript
document.getElementById('runInitialChunkBtn').addEventListener('click', async function() {
  const resultDiv = document.getElementById('initialChunkResult');
  const modelInput = document.getElementById('chunkingModel').value.trim();

  if (!modelInput) {
    resultDiv.innerHTML = '<p style="color: red;">‚ö†Ô∏è Veuillez sp√©cifier un mod√®le</p>';
    return;
  }

  resultDiv.innerHTML = '<p>‚è≥ Chunking en cours (peut prendre plusieurs minutes)...</p>';

  try {
    // Cr√©er FormData avec le mod√®le
    const formData = new FormData();
    formData.append('model', modelInput);

    const response = await fetch('/initial_text_chunking', {
      method: 'POST',
      body: formData
    });

    const data = await response.json();

    // V√©rifier si un fallback est n√©cessaire
    if (data.fallback_needed) {
      // Afficher le modal de confirmation
      showFallbackModal(data.error);
    } else if (data.success) {
      resultDiv.innerHTML = '<p style="color: green;">‚úì Chunking initial r√©ussi avec OpenRouter !</p>';
      // Activer la section suivante
      document.getElementById('dense-embedding-section').style.display = 'block';
    } else {
      resultDiv.innerHTML = `<p style="color: red;">‚úó Erreur : ${data.message || 'Erreur inconnue'}</p>`;
    }
  } catch (error) {
    resultDiv.innerHTML = `<p style="color: red;">‚úó Erreur r√©seau : ${error.message}</p>`;
  }
});

// Fonction pour afficher le modal de fallback
function showFallbackModal(errorMessage) {
  const modal = document.getElementById('fallbackModal');
  const errorMsgElement = document.getElementById('fallbackErrorMessage');

  errorMsgElement.textContent = errorMessage || "Impossible de se connecter √† OpenRouter";
  modal.style.display = 'flex';
}

// Bouton "Confirmer" dans le modal
document.getElementById('confirmFallback').addEventListener('click', async function() {
  const modal = document.getElementById('fallbackModal');
  const resultDiv = document.getElementById('initialChunkResult');

  modal.style.display = 'none';
  resultDiv.innerHTML = '<p>‚è≥ Chunking avec OpenAI (fallback) en cours...</p>';

  try {
    const response = await fetch('/initial_text_chunking_fallback', {
      method: 'POST'
    });

    const data = await response.json();

    if (data.success) {
      resultDiv.innerHTML = '<p style="color: green;">‚úì Chunking r√©ussi avec OpenAI (fallback) !</p>';
      document.getElementById('dense-embedding-section').style.display = 'block';
    } else {
      resultDiv.innerHTML = `<p style="color: red;">‚úó Erreur fallback : ${data.message || 'Erreur inconnue'}</p>`;
    }
  } catch (error) {
    resultDiv.innerHTML = `<p style="color: red;">‚úó Erreur r√©seau : ${error.message}</p>`;
  }
});

// Bouton "Annuler" dans le modal
document.getElementById('cancelFallback').addEventListener('click', function() {
  const modal = document.getElementById('fallbackModal');
  const resultDiv = document.getElementById('initialChunkResult');

  modal.style.display = 'none';
  resultDiv.innerHTML = '<p style="color: orange;">‚ö†Ô∏è Chunking annul√© par l\'utilisateur</p>';
});
```

---

## üß™ Tests & Validation

### Tests unitaires (√† cr√©er)

**Fichier** : `tests/test_openrouter_chunking.py`

```python
import unittest
from scripts.rad_chunk import gpt_recode_batch_with_openrouter
from unittest.mock import patch, MagicMock

class TestOpenRouterChunking(unittest.TestCase):

    @patch('scripts.rad_chunk.client_openrouter')
    def test_openrouter_success(self, mock_client):
        """Test appel r√©ussi vers OpenRouter"""
        # Mock response
        mock_response = MagicMock()
        mock_response.choices[0].message.content = "Texte nettoy√©"
        mock_client.chat.completions.create.return_value = mock_response

        success, texts, error = gpt_recode_batch_with_openrouter(
            ["Texte brut OCR"],
            "Instructions de nettoyage"
        )

        self.assertTrue(success)
        self.assertEqual(texts[0], "Texte nettoy√©")
        self.assertIsNone(error)

    @patch('scripts.rad_chunk.client_openrouter')
    def test_openrouter_failure(self, mock_client):
        """Test √©chec OpenRouter (retourne erreur)"""
        mock_client.chat.completions.create.side_effect = Exception("API Error")

        success, texts, error = gpt_recode_batch_with_openrouter(
            ["Texte brut"],
            "Instructions"
        )

        self.assertFalse(success)
        self.assertIsNone(texts)
        self.assertIn("API Error", error)

    def test_openrouter_client_not_initialized(self):
        """Test si client OpenRouter non initialis√©"""
        # √Ä tester avec OPENROUTER_API_KEY vide
        pass
```

---

### Tests d'int√©gration

#### Sc√©nario 1 : OpenRouter accessible
1. Configurer `.env` avec `OPENROUTER_API_KEY` valide
2. Uploader `output.csv` dans l'UI
3. Entrer `openai/gemini-2.5-flash` dans le champ mod√®le
4. Cliquer "Generate Chunks"
5. **Attendu** : Chunking r√©ussi, message vert, section 3.2 activ√©e

#### Sc√©nario 2 : OpenRouter inaccessible (cl√© invalide)
1. Configurer `.env` avec `OPENROUTER_API_KEY` invalide
2. Uploader `output.csv`
3. Cliquer "Generate Chunks"
4. **Attendu** : Modal de confirmation affich√© avec message d'erreur
5. Cliquer "Oui, utiliser OpenAI"
6. **Attendu** : Chunking r√©ussi avec OpenAI, message vert avec "(fallback)"

#### Sc√©nario 3 : Annulation fallback
1. Simuler erreur OpenRouter
2. Modal affich√©
3. Cliquer "Annuler"
4. **Attendu** : Message orange "Chunking annul√©", section 3.2 reste cach√©e

#### Sc√©nario 4 : Mod√®le personnalis√©
1. Entrer `anthropic/claude-3.5-sonnet` dans le champ mod√®le
2. Lancer chunking
3. **Attendu** : Utilise Claude au lieu de Gemini

---

### Tests manuels (checklist)

- [ ] **Cl√© OpenRouter manquante** : V√©rifier fallback imm√©diat sans modal
- [ ] **Timeout r√©seau** : Simuler avec `--timeout 1` ‚Üí modal affich√©
- [ ] **Quota OpenRouter d√©pass√©** : Tester avec compte gratuit satur√©
- [ ] **Logs** : V√©rifier que `chunking.log` distingue OpenRouter/OpenAI
- [ ] **Console** : Messages `‚úì OpenRouter` ou `‚Üí Utilisation OpenAI` affich√©s
- [ ] **Persistence** : Recharger la page ‚Üí cl√© OpenRouter masqu√©e correctement

---

## üìñ Documentation √† mettre √† jour

### 1. README.md

**Section √† ajouter/modifier** : "Configuration"

```markdown
### Variables d'environnement

Cr√©ez un fichier `.env` √† la racine du projet avec les cl√©s suivantes :

```bash
# OpenAI (pour embeddings)
OPENAI_API_KEY=sk-...

# OpenRouter (pour chunking/recodage de texte)
OPENROUTER_API_KEY=sk-or-v1-...
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_DEFAULT_MODEL=openai/gemini-2.5-flash

# Autres services (Pinecone, Mistral, etc.)
# ...
```

**Comportement du chunking** :
- Par d√©faut, le syst√®me utilise **OpenRouter avec Gemini 2.5 Flash** pour le recodage de texte
- Si OpenRouter est inaccessible (erreur r√©seau, cl√© invalide, quota d√©pass√©), le syst√®me demande confirmation pour basculer sur **OpenAI (gpt-4o-mini)**
- Les embeddings continuent d'utiliser **OpenAI (text-embedding-3-large)**

**Mod√®les OpenRouter support√©s** :
- `openai/gemini-2.5-flash` (recommand√©, par d√©faut)
- `anthropic/claude-3.5-sonnet`
- `openai/gpt-4o`
- Voir la liste compl√®te : https://openrouter.ai/models
```

---

### 2. .claude/AGENTS.md

**Section √† modifier** : "Agent `rad_chunk.py`" (ligne 92-135)

**Modifications** :

1. **Ligne 101** : Ajouter OpenRouter dans les d√©pendances

```markdown
### D√©pendances & environnement
- `OPENAI_API_KEY` obligatoire pour embeddings (text-embedding-3-large)
- `OPENROUTER_API_KEY` optionnel pour recodage GPT (Gemini 2.5 Flash via OpenRouter). Si absent, fallback sur OpenAI.
- Librairies: `langchain_text_splitters`, `openai`, `spacy` (`fr_core_news_md` t√©l√©charg√© si absent), `tqdm`, `pandas`.
```

2. **Ligne 108** : Documenter le nouveau param√®tre `--model`

```markdown
### Param√®tres CLI
```bash
python scripts/rad_chunk.py \
  --input sources/MaBiblio/output.csv \
  --output sources/MaBiblio \
  --phase all \
  --model openai/gemini-2.5-flash  # Optionnel, d√©faut: OPENROUTER_DEFAULT_MODEL
```
- `--input` : CSV (phase `initial`) ou JSON (phases `dense`/`sparse`).
- `--output` : dossier cible des JSON (cr√©√© si besoin).
- `--phase` : `initial`, `dense`, `sparse`, ou `all` (encha√Æne les trois).
- `--model` : Mod√®le OpenRouter pour le recodage (format `provider/model-name`). Si absent, utilise `OPENROUTER_DEFAULT_MODEL` du .env, sinon fallback OpenAI.
```

3. **Ligne 119** : Documenter le comportement OpenRouter

```markdown
### D√©tails par phase
- **initial** :
  - Lit un CSV, d√©coupe le champ `texteocr` en chunks (~2 500 tokens avec chevauchement 250)
  - **Recodage via OpenRouter** (Gemini 2.5 Flash par d√©faut) si `OPENROUTER_API_KEY` est configur√©e
  - **Fallback OpenAI** (gpt-4o-mini) si OpenRouter √©choue ou si l'utilisateur annule
  - Saute le recodage si l'OCR provient de Mistral (chunks Markdown d√©j√† propres)
  - Sauvegarde `output_chunks.json`
- **dense** : attend un fichier `_chunks.json`, g√©n√®re les embeddings denses OpenAI (`text-embedding-3-large`), √©crit `_chunks_with_embeddings.json`.
- **sparse** : attend `_chunks_with_embeddings.json`, d√©rive les features spaCy (POS filtr√©s, lemmas, TF normalis√©, hachage mod 100 000), sauvegarde `_chunks_with_embeddings_sparse.json`.
- **all** : encha√Æne les trois sous-√©tapes avec journalisation dans `<output>/chunking.log`.
```

4. **Ligne 130** : Ajouter section troubleshooting

```markdown
### Comportement compl√©mentaire
- Si la cl√© OpenAI est absente, le script la demande et propose de la stocker via `python-dotenv`.
- **M√©canisme de fallback OpenRouter ‚Üí OpenAI** :
  1. Le script tente d'utiliser OpenRouter en premier
  2. En cas d'erreur (r√©seau, authentification, quota), une exception `OPENROUTER_UNAVAILABLE` est lev√©e
  3. L'interface web affiche un modal de confirmation
  4. Si l'utilisateur confirme, le chunking est relanc√© avec OpenAI
  5. Si l'utilisateur annule, le processus s'arr√™te
- SpaCy : tronque les textes tr√®s longs √† `nlp.max_length` (ou 50 000 caract√®res) pour √©viter les d√©passements.
- Les identifiants de chunk incluent `doc_id`, `chunk_index`, `total_chunks` pour faciliter l'upload.
- Les erreurs d'API GPT sont r√©essay√©es s√©quentiellement (seconde passe) avant fallback sur le texte brut.
```

---

### 3. Nouveau fichier : .claude/OPENROUTER.md

**Cr√©er un nouveau fichier** avec le contenu suivant :

```markdown
# Guide OpenRouter pour RAGpy

## Qu'est-ce qu'OpenRouter ?

OpenRouter est un service qui fournit un **acc√®s unifi√© √† plusieurs mod√®les de langage** (OpenAI, Anthropic, Google, Meta, etc.) via une seule API.

**Avantages** :
- Un seul compte pour acc√©der √† Gemini, Claude, GPT-4, Llama, etc.
- Tarification comp√©titive (souvent moins cher que les APIs officielles)
- Basculement facile entre mod√®les sans changer de code
- Pas besoin de comptes s√©par√©s chez chaque fournisseur

---

## Configuration dans RAGpy

### 1. Cr√©er un compte OpenRouter

1. Aller sur https://openrouter.ai/
2. S'inscrire (gratuit)
3. Aller dans "Keys" ‚Üí "Create New Key"
4. Copier la cl√© (format : `sk-or-v1-...`)

### 2. Ajouter la cl√© dans RAGpy

**Option A : Fichier .env**
```bash
OPENROUTER_API_KEY=sk-or-v1-xxxxxxxxxxxxx
OPENROUTER_DEFAULT_MODEL=openai/gemini-2.5-flash
```

**Option B : Interface web**
1. Lancer RAGpy : `uvicorn app.main:app --reload --host 0.0.0.0 --port 8000`
2. Ouvrir http://localhost:8000
3. Cliquer sur "Settings" (‚öôÔ∏è)
4. Remplir "OpenRouter API Key"
5. Cliquer "Save"

---

## Mod√®les recommand√©s pour le chunking

| Mod√®le | Format OpenRouter | Co√ªt (par 1M tokens) | Performance |
|--------|-------------------|----------------------|-------------|
| **Gemini 2.5 Flash** | `openai/gemini-2.5-flash` | ~$0.15 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Recommand√© |
| Claude 3.5 Sonnet | `anthropic/claude-3.5-sonnet` | ~$3.00 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Tr√®s bon |
| GPT-4o mini | `openai/gpt-4o-mini` | ~$0.30 | ‚≠ê‚≠ê‚≠ê‚≠ê Bon |
| Llama 3.3 70B | `meta-llama/llama-3.3-70b-instruct` | ~$0.50 | ‚≠ê‚≠ê‚≠ê Moyen |

**Pour le chunking de texte OCR**, Gemini 2.5 Flash est le meilleur compromis co√ªt/qualit√©.

---

## Utilisation dans l'interface

### Chunking avec OpenRouter

1. Uploader `output.csv` dans la section 3.1
2. Le champ "Mod√®le de recodage" est pr√©-rempli avec `openai/gemini-2.5-flash`
3. (Optionnel) Changer le mod√®le si souhait√©
4. Cliquer "Generate Chunks"

**Le syst√®me tentera d'utiliser OpenRouter automatiquement.**

### Si OpenRouter est inaccessible

Un modal s'affichera :
```
‚ö†Ô∏è OpenRouter inaccessible
[Message d'erreur]

Voulez-vous utiliser OpenAI (gpt-4o-mini) √† la place ?

[Annuler]  [Oui, utiliser OpenAI]
```

- **Oui** ‚Üí Le chunking continue avec OpenAI
- **Annuler** ‚Üí Le processus s'arr√™te

---

## Troubleshooting

### Erreur : "OPENROUTER_API_KEY non configur√©e"

**Solution** : Ajouter la cl√© dans `.env` ou via Settings

### Erreur : "Unauthorized" / HTTP 401

**Causes possibles** :
- Cl√© API invalide
- Cl√© expir√©e
- Cr√©dit OpenRouter √©puis√©

**Solution** :
1. V√©rifier la cl√© sur https://openrouter.ai/keys
2. V√©rifier le solde du compte
3. Reg√©n√©rer une nouvelle cl√© si n√©cessaire

### Erreur : "Rate limit exceeded" / HTTP 429

**Cause** : Quota OpenRouter d√©pass√© (trop de requ√™tes)

**Solution** :
- Attendre quelques minutes
- Augmenter le cr√©dit du compte
- Utiliser le fallback OpenAI

### Erreur : "Model not found" / HTTP 404

**Cause** : Format de mod√®le invalide

**Solution** : V√©rifier le format sur https://openrouter.ai/models
- Correct : `openai/gemini-2.5-flash`
- Incorrect : `gemini-2.5-flash` (manque le provider)

### Timeout / Erreur r√©seau

**Cause** : Connexion OpenRouter lente ou indisponible

**Solution** :
- V√©rifier la connexion internet
- Utiliser le fallback OpenAI
- V√©rifier le statut : https://status.openrouter.ai/

---

## Comparaison de co√ªts

### Recodage de 1000 chunks (moyenne 500 tokens par chunk)

| Fournisseur | Mod√®le | Co√ªt estim√© |
|-------------|--------|-------------|
| **OpenRouter** | Gemini 2.5 Flash | **~$0.08** |
| OpenRouter | Claude 3.5 Sonnet | ~$1.50 |
| OpenRouter | GPT-4o mini | ~$0.15 |
| **OpenAI direct** | gpt-4o-mini | ~$0.20 |

**√âconomie avec OpenRouter (Gemini)** : ~60% par rapport √† OpenAI direct

---

## Logs et d√©bogage

Les logs du chunking incluent des messages distincts :

**Succ√®s OpenRouter** :
```
‚úì Client OpenRouter initialis√© (mod√®le par d√©faut : openai/gemini-2.5-flash)
‚Üí Tentative de recodage via OpenRouter (openai/gemini-2.5-flash)...
‚úì Recodage OpenRouter r√©ussi (150 chunks)
```

**Fallback OpenAI** :
```
‚úó Erreur globale OpenRouter: Unauthorized
‚Üí Utilisation d'OpenAI (gpt-4o-mini) pour le recodage...
```

Consultez les logs :
- Console : stdout en temps r√©el
- Fichier : `uploads/<session>/chunking.log`

---

## Ressources

- Documentation officielle : https://openrouter.ai/docs
- Liste des mod√®les : https://openrouter.ai/models
- Pricing : https://openrouter.ai/models?pricing=true
- Status : https://status.openrouter.ai/
```

---

## üì¶ Checklist post-d√©veloppement

### Configuration
- [ ] Cr√©er fichier `.env.example` avec :
  ```bash
  OPENAI_API_KEY=sk-...
  OPENROUTER_API_KEY=sk-or-v1-...
  OPENROUTER_DEFAULT_MODEL=openai/gemini-2.5-flash
  ```
- [ ] V√©rifier `.gitignore` inclut `.env`

### Code
- [ ] Tester avec `.env` minimal (sans OPENROUTER_API_KEY)
  - **Attendu** : Fallback imm√©diat sur OpenAI
- [ ] Tester avec OPENROUTER_API_KEY valide
  - **Attendu** : Utilisation d'OpenRouter
- [ ] Tester avec OPENROUTER_API_KEY invalide
  - **Attendu** : Modal de confirmation, puis fallback sur OpenAI si confirm√©

### Logs
- [ ] V√©rifier que les logs distinguent OpenRouter/OpenAI
- [ ] Console affiche `‚úì OpenRouter` ou `‚Üí OpenAI` clairement
- [ ] Fichier `chunking.log` contient les messages d'erreur complets

### Documentation
- [ ] README.md mis √† jour avec variables OpenRouter
- [ ] AGENTS.md mis √† jour avec comportement fallback
- [ ] OPENROUTER.md cr√©√© avec guide complet
- [ ] Ajouter section "Migration" dans README si n√©cessaire

### Interface
- [ ] Champ mod√®le pr√©-rempli avec `openai/gemini-2.5-flash`
- [ ] Modal fallback s'affiche correctement
- [ ] Boutons "Confirmer" / "Annuler" fonctionnent
- [ ] Settings permet de sauvegarder/charger la cl√© OpenRouter

### Tests
- [ ] Tests unitaires passent (si cr√©√©s)
- [ ] Tests d'int√©gration pour les 4 sc√©narios
- [ ] Validation manuelle des erreurs (timeout, quota, etc.)

### D√©ploiement
- [ ] Red√©marrer le serveur : `uvicorn app.main:app --reload`
- [ ] Tester un chunking complet de bout en bout
- [ ] V√©rifier que les fichiers JSON produits sont identiques (OpenRouter vs OpenAI)

---

## üéØ Ordre d'impl√©mentation recommand√©

### Phase 1 : Backend (rad_chunk.py)
1. ‚úÖ Ajouter variables .env (OPENROUTER_API_KEY, etc.)
2. ‚úÖ Cr√©er client OpenRouter (apr√®s ligne 70)
3. ‚úÖ Impl√©menter `gpt_recode_batch_with_openrouter()`
4. ‚úÖ Ajouter argument `--model` au parser CLI
5. ‚úÖ Modifier `process_document_chunks()` pour utiliser OpenRouter
6. ‚úÖ Modifier `main()` pour passer le mod√®le

### Phase 2 : Backend (app/main.py)
7. ‚úÖ Cr√©er endpoint `/check_openrouter`
8. ‚úÖ Modifier `/initial_text_chunking` (ajouter param√®tre `model`, g√©rer erreur)
9. ‚úÖ Cr√©er endpoint `/initial_text_chunking_fallback`
10. ‚úÖ Modifier `/get_credentials` (ajouter OpenRouter)
11. ‚úÖ Modifier `/save_credentials` (sauvegarder OpenRouter)

### Phase 3 : Frontend (index.html)
12. ‚úÖ Ajouter champ mod√®le dans section 3.1
13. ‚úÖ Cr√©er modal de confirmation fallback
14. ‚úÖ Ajouter OpenRouter dans Settings
15. ‚úÖ JavaScript : charger credentials OpenRouter
16. ‚úÖ JavaScript : sauvegarder credentials OpenRouter
17. ‚úÖ JavaScript : g√©rer bouton "Generate Chunks" + modal

### Phase 4 : Tests & Documentation
18. ‚úÖ Tests unitaires (optionnel mais recommand√©)
19. ‚úÖ Tests d'int√©gration (4 sc√©narios)
20. ‚úÖ Mettre √† jour README.md
21. ‚úÖ Mettre √† jour AGENTS.md
22. ‚úÖ Cr√©er OPENROUTER.md

### Phase 5 : Validation finale
23. ‚úÖ Checklist post-dev compl√®te
24. ‚úÖ Test de bout en bout (upload CSV ‚Üí chunking ‚Üí embeddings ‚Üí DB)
25. ‚úÖ V√©rification des logs et console

---

## üìù Notes importantes

### Format des mod√®les OpenRouter

OpenRouter utilise le format `provider/model-name` :
- ‚úÖ Correct : `openai/gemini-2.5-flash`
- ‚úÖ Correct : `anthropic/claude-3.5-sonnet`
- ‚ùå Incorrect : `gemini-2.5-flash` (manque le provider)
- ‚ùå Incorrect : `gpt-4o-mini` (ce n'est pas un mod√®le OpenRouter, mais OpenAI direct)

### Headers requis pour OpenRouter

L'API OpenRouter requiert des headers sp√©cifiques :
```python
extra_headers={
    "HTTP-Referer": "https://ragpy.local",  # Obligatoire
    "X-Title": "RAGpy Chunking Pipeline"     # Optionnel mais recommand√©
}
```

### Gestion du fallback

**D√©clencheurs de fallback** (toute erreur OpenRouter) :
- Erreurs r√©seau (timeout, connexion refus√©e)
- Erreurs HTTP 4xx (authentification, quota)
- Erreurs HTTP 5xx (serveur OpenRouter down)
- Exceptions Python (malformation requ√™te, etc.)

**Processus** :
1. Erreur d√©tect√©e ‚Üí exception `OPENROUTER_UNAVAILABLE` lev√©e
2. Backend FastAPI capture l'exception ‚Üí retourne `{fallback_needed: true}`
3. Frontend affiche modal de confirmation
4. Si confirm√© ‚Üí appel `/initial_text_chunking_fallback` (sans `--model`)
5. Si annul√© ‚Üí processus stopp√©

### Compatibilit√© embeddings

**Les embeddings continuent d'utiliser OpenAI** :
- Mod√®le : `text-embedding-3-large`
- Fonction : `get_embeddings_batch()` (ligne 301 de rad_chunk.py)
- **Aucune modification n√©cessaire** pour cette partie

---

## üîç Points de vigilance

### S√©curit√©
- Ne jamais commit `.env` dans Git
- Masquer les cl√©s API dans l'UI (afficher seulement les 20 premiers caract√®res)
- Valider les inputs utilisateur (mod√®le, cl√©s API)

### Performance
- OpenRouter peut √™tre plus lent qu'OpenAI direct (latence r√©seau suppl√©mentaire)
- Pr√©voir des timeouts appropri√©s (actuellement 3600s = 1h)
- Logger les temps de r√©ponse pour comparaison

### Co√ªts
- V√©rifier les prix sur https://openrouter.ai/models?pricing=true
- Gemini 2.5 Flash est g√©n√©ralement le moins cher (~$0.15/1M tokens)
- Comparer avec OpenAI direct (gpt-4o-mini ~$0.30/1M tokens)

### UX
- Expliquer clairement le fallback √† l'utilisateur
- Afficher les messages d'erreur de mani√®re compr√©hensible
- Permettre l'annulation √† tout moment

---

## ‚úÖ Crit√®res de succ√®s

Le d√©veloppement sera consid√©r√© comme r√©ussi si :

1. **Fonctionnel** :
   - [ ] Chunking avec OpenRouter fonctionne (succ√®s)
   - [ ] Fallback vers OpenAI fonctionne (en cas d'erreur)
   - [ ] Modal de confirmation s'affiche et fonctionne
   - [ ] Annulation du processus fonctionne

2. **Configuration** :
   - [ ] Variables .env charg√©es correctement
   - [ ] Settings UI permet de configurer OpenRouter
   - [ ] Cl√©s masqu√©es dans l'UI mais sauvegard√©es en entier

3. **Robustesse** :
   - [ ] G√®re les erreurs r√©seau (timeout, connexion)
   - [ ] G√®re les erreurs API (401, 429, 500)
   - [ ] Logs clairs et exploitables
   - [ ] Pas de r√©gression sur les embeddings (toujours OpenAI)

4. **Documentation** :
   - [ ] README.md √† jour
   - [ ] AGENTS.md √† jour
   - [ ] OPENROUTER.md cr√©√© et complet
   - [ ] Code comment√© (surtout les parties critiques)

5. **Tests** :
   - [ ] Les 4 sc√©narios d'int√©gration passent
   - [ ] Tests manuels concluants
   - [ ] Validation de bout en bout (CSV ‚Üí chunks ‚Üí embeddings ‚Üí DB)

---

## üìû Support

En cas de probl√®me :
1. Consulter `chunking.log` dans le dossier de sortie
2. V√©rifier les logs du serveur FastAPI (console ou `ragpy_server.log`)
3. Tester la cl√© OpenRouter sur https://openrouter.ai/playground
4. V√©rifier le statut d'OpenRouter : https://status.openrouter.ai/

---

**Fin du plan de d√©veloppement**
