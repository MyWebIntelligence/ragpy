# Architecture actuelle du pipeline RAGpy

**Date de crÃ©ation** : 2025-10-21
**DerniÃ¨re mise Ã  jour** : 2025-01-25 (ajout app/main.py, ingestion CSV, OpenRouter)
**Objectif** : Documenter l'architecture existante complÃ¨te

---

## Vue d'ensemble du flux de donnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            PIPELINE COMPLET RAGpy (Mise Ã  jour 2025-01-25)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

0. app/main.py : Serveur FastAPI (ORCHESTRATEUR WEB)
   â”œâ”€ Interface utilisateur : templates/index.html
   â”œâ”€ Gestion de sessions : uploads/<session>/
   â”œâ”€ Endpoints REST pour chaque Ã©tape du pipeline
   â”œâ”€ Orchestration des scripts 1-3 via subprocess
   â””â”€ Gestion credentials (.env) et configuration

1. INGESTION (2 sources implÃ©mentÃ©es)
   â”œâ”€ A) rad_dataframe.py : Zotero JSON + PDF â†’ CSV (output.csv)
   â”‚     â”œâ”€ Input : Zotero JSON + rÃ©pertoire PDF
   â”‚     â”œâ”€ OCR   : Mistral â†’ OpenAI â†’ PyMuPDF (legacy)
   â”‚     â””â”€ Output: CSV avec texteocr + texteocr_provider + mÃ©tadonnÃ©es
   â”‚
   â””â”€ B) ingestion/csv_ingestion.py : CSV direct â†’ List[Document]
         â”œâ”€ Input : CSV avec colonne texte personnalisable
         â”œâ”€ Mapping : colonne â†’ texteocr
         â”œâ”€ PrÃ©servation : toutes les mÃ©tadonnÃ©es CSV
         â””â”€ Output: List[Document] (core/document.py)

2. rad_chunk.py : CSV/Documents â†’ JSON avec chunks + embeddings
   â”œâ”€ Phase initial : texteocr â†’ chunks JSON (avec recodage GPT/OpenRouter)
   â”œâ”€ Phase dense   : chunks â†’ chunks + embeddings denses (OpenAI)
   â””â”€ Phase sparse  : chunks â†’ chunks + embeddings sparses (spaCy)

3. rad_vectordb.py : JSON â†’ Base vectorielle
   â”œâ”€ Pinecone  : upsert avec mÃ©tadonnÃ©es
   â”œâ”€ Weaviate  : insert avec mÃ©tadonnÃ©es
   â””â”€ Qdrant    : upsert avec mÃ©tadonnÃ©es

Utilitaires:
- scripts/crawl.py : Web crawler â†’ PDF/Markdown (documentation en ligne)
```

---

## 0. Module `app/main.py` â€” Orchestrateur FastAPI (POINT D'ENTRÃ‰E)

### ResponsabilitÃ©s

**app/main.py (1042 lignes)** est le **point d'entrÃ©e principal** de l'application. C'est un serveur FastAPI qui :
- Expose une interface web complÃ¨te (templates/index.html)
- Orchestre les 3 scripts du pipeline via subprocess
- GÃ¨re les sessions utilisateur et les uploads
- Fournit des endpoints REST pour chaque phase
- GÃ¨re la configuration (.env) et les credentials

### Architecture FastAPI

#### Structure des rÃ©pertoires

```python
# Lignes 23-28 : DÃ©finition des chemins
APP_DIR = os.path.dirname(os.path.abspath(__file__))  # .../ragpy/app
RAGPY_DIR = os.path.dirname(APP_DIR)                  # .../ragpy
LOG_DIR = os.path.join(RAGPY_DIR, "logs")
UPLOAD_DIR = os.path.join(RAGPY_DIR, "uploads")
STATIC_DIR = os.path.join(APP_DIR, "static")
TEMPLATES_DIR = os.path.join(APP_DIR, "templates")
```

Chaque session utilisateur obtient un rÃ©pertoire unique dans `uploads/<session>/` contenant :
- Fichiers uploadÃ©s (ZIP, CSV)
- Fichiers intermÃ©diaires (output.csv, output_chunks.json, etc.)
- Logs spÃ©cifiques (chunking.log, dense_embedding.log, etc.)

#### Endpoints principaux

| Endpoint | MÃ©thode | RÃ´le | Script appelÃ© |
|----------|---------|------|---------------|
| `/` | GET | Interface web principale | - |
| `/upload_zip` | POST | Upload ZIP Zotero + extraction | - |
| `/upload_csv_direct` | POST | Upload CSV direct (ingestion CSV) | - |
| `/process_dataframe` | POST | Extraction PDF/OCR | `rad_dataframe.py` |
| `/initial_text_chunking` | POST | GÃ©nÃ©ration des chunks | `rad_chunk.py --phase initial` |
| `/dense_embedding_generation` | POST | Embeddings denses | `rad_chunk.py --phase dense` |
| `/sparse_embedding_generation` | POST | Embeddings sparses | `rad_chunk.py --phase sparse` |
| `/insert_to_vectordb` | POST | Insertion base vectorielle | `rad_vectordb.py` |
| `/get_credentials` | GET | RÃ©cupÃ¨re credentials du .env | - |
| `/save_credentials` | POST | Sauvegarde credentials dans .env | - |

#### Flux utilisateur typique (ZIP Zotero)

```python
# 1. Upload ZIP (ligne 72-140)
POST /upload_zip
â”œâ”€ Extraction ZIP dans uploads/<session>/
â”œâ”€ Recherche Zotero JSON
â””â”€ Retour: {"session": "<session>", "files": [...]}

# 2. Traitement DataFrame/OCR (ligne 538-605)
POST /process_dataframe
â”œâ”€ subprocess.run(["python3", "rad_dataframe.py", ...])
â”œâ”€ Input:  uploads/<session>/*.json + PDFs
â””â”€ Output: uploads/<session>/output.csv

# 3. Chunking initial (ligne 656-698) - NOUVEAU: Support OpenRouter
POST /initial_text_chunking?model=openai/gemini-2.5-flash
â”œâ”€ subprocess.run(["python3", "rad_chunk.py", "--phase", "initial", "--model", model])
â”œâ”€ Input:  uploads/<session>/output.csv
â””â”€ Output: uploads/<session>/output_chunks.json

# 4. Embeddings denses (ligne 700-755)
POST /dense_embedding_generation
â”œâ”€ subprocess.run(["python3", "rad_chunk.py", "--phase", "dense"])
â”œâ”€ Input:  uploads/<session>/output_chunks.json
â””â”€ Output: uploads/<session>/output_chunks_with_embeddings.json

# 5. Embeddings sparses (ligne 757-812)
POST /sparse_embedding_generation
â”œâ”€ subprocess.run(["python3", "rad_chunk.py", "--phase", "sparse"])
â”œâ”€ Input:  uploads/<session>/output_chunks_with_embeddings.json
â””â”€ Output: uploads/<session>/output_chunks_with_embeddings_sparse.json

# 6. Insertion vectorielle (ligne 814-889)
POST /insert_to_vectordb
â”œâ”€ subprocess.run(["python3", "rad_vectordb.py", "--db", db_choice, ...])
â”œâ”€ Input:  uploads/<session>/output_chunks_with_embeddings_sparse.json
â””â”€ Output: Insertion dans Pinecone/Weaviate/Qdrant
```

#### Gestion des credentials (NOUVEAU 2025-01-25)

```python
# Ligne 486-536 : GET /get_credentials
# Lit le fichier .env et retourne les clÃ©s API (masquÃ©es)
credentials = {
    "OPENAI_API_KEY": "sk-...",
    "OPENROUTER_API_KEY": "sk-or-v1-...",      # NOUVEAU
    "OPENROUTER_DEFAULT_MODEL": "openai/gemini-2.5-flash",  # NOUVEAU
    "PINECONE_API_KEY": "pcsk-...",
    # ... autres credentials
}

# Ligne 538-601 : POST /save_credentials
# Sauvegarde les credentials dans le fichier .env
# Support OpenRouter ajoutÃ© le 2025-01-25
```

### Logging centralisÃ©

```python
# Ligne 34-50 : Configuration logging
log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')
app_log_file = os.path.join(LOG_DIR, "app.log")
file_handler = RotatingFileHandler(app_log_file, maxBytes=10*1024*1024, backupCount=5)

# Logs consolidÃ©s :
# - logs/app.log : Toutes les requÃªtes FastAPI + erreurs subprocess
# - uploads/<session>/chunking.log : Logs de rad_chunk.py
# - uploads/<session>/dense_embedding.log : Logs embeddings denses
# - uploads/<session>/sparse_embedding.log : Logs embeddings sparses
```

### Gestion des erreurs subprocess

Tous les endpoints qui appellent des scripts Python gÃ¨rent 3 types d'erreurs :
1. **TimeoutExpired** : Script > 1 heure â†’ Code 504
2. **CalledProcessError** : Script retourne code != 0 â†’ Code 500 avec dÃ©tails
3. **Exception gÃ©nÃ©rique** : Erreur inattendue â†’ Code 500 avec traceback

### Variables d'environnement

L'application lit et Ã©crit dans le fichier `.env` Ã  la racine de `ragpy/` :

```bash
# Obligatoire
OPENAI_API_KEY

# Nouveaux (2025-01-25)
OPENROUTER_API_KEY              # Optionnel - Alternative Ã©conomique
OPENROUTER_DEFAULT_MODEL        # Optionnel - ModÃ¨le par dÃ©faut

# Bases vectorielles (au moins 1)
PINECONE_API_KEY
PINECONE_ENV
WEAVIATE_URL
WEAVIATE_API_KEY
QDRANT_URL
QDRANT_API_KEY
```

### DÃ©marrage du serveur

```bash
# Ligne 1040+ : Point d'entrÃ©e
# uvicorn app.main:app --reload --host 0.0.0.0
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Le serveur dÃ©marre sur `http://localhost:8000` avec live reload en dÃ©veloppement.

---

## 1A. Module `core/document.py` â€” Classe Document unifiÃ©e (IMPLÃ‰MENTÃ‰)

### ResponsabilitÃ©s

DÃ©finit la **structure de donnÃ©es commune** Ã  toutes les sources d'ingestion (PDF/OCR, CSV, futures sources). Garantit l'uniformitÃ© du pipeline.

### Structure de la classe

```python
# Ligne 16-68 : Classe Document
@dataclass
class Document:
    """
    ReprÃ©sentation unifiÃ©e d'un document dans le pipeline RAGpy.

    Attributs:
        texteocr (str): Contenu textuel du document. Variable pivot unique
                        pour tout le pipeline. Ne peut pas Ãªtre vide.
        meta (Dict[str, Any]): MÃ©tadonnÃ©es arbitraires associÃ©es au document.
                               Peut contenir n'importe quels champs selon la source.
        source_type (str): Type de source d'ingestion ("pdf", "csv", etc.).
                          AjoutÃ© automatiquement dans meta si absent.
    """
    texteocr: str
    meta: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """Validation et normalisation automatique."""
        # Validation texteocr non vide
        if not self.texteocr or not self.texteocr.strip():
            raise ValueError("Le champ 'texteocr' ne peut pas Ãªtre vide")

        # Ajouter source_type dans meta si absent
        if "source_type" not in self.meta:
            self.meta["source_type"] = "unknown"

    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dictionnaire (pour compatibilitÃ© rad_chunk.py)."""
        return {
            "texteocr": self.texteocr,
            **self.meta
        }
```

### Exemples d'utilisation

```python
# Document issu d'un PDF via OCR
doc_pdf = Document(
    texteocr="Texte extrait du PDF...",
    meta={
        "title": "Article scientifique",
        "authors": "Smith, J.",
        "filename": "article.pdf",
        "source_type": "pdf",
        "texteocr_provider": "mistral"
    }
)

# Document issu d'un CSV
doc_csv = Document(
    texteocr="Contenu de la colonne 'text'",
    meta={
        "title": "Titre article",
        "category": "Science",
        "url": "https://...",
        "source_type": "csv",
        "row_index": 42
    }
)
```

### Avantages de cette abstraction

1. **UniformitÃ©** : Toutes les sources produisent le mÃªme type d'objet
2. **Validation** : Impossible de crÃ©er un Document avec texteocr vide
3. **ExtensibilitÃ©** : `meta` accepte n'importe quelle mÃ©tadonnÃ©e
4. **CompatibilitÃ©** : `to_dict()` permet l'intÃ©gration avec rad_chunk.py

---

## 1B. Module `ingestion/csv_ingestion.py` â€” Ingestion CSV (IMPLÃ‰MENTÃ‰)

### ResponsabilitÃ©s

Permet d'injecter des fichiers CSV dans le pipeline RAGpy en **contournant l'Ã©tape OCR**. Mappe une colonne CSV vers la variable pivot `texteocr` et conserve toutes les autres colonnes comme mÃ©tadonnÃ©es.

### Classes principales

#### `CSVIngestionConfig` (ligne 36-76)

Configuration pour personnaliser l'ingestion CSV :

```python
config = CSVIngestionConfig(
    text_column="description",      # Colonne source du texte
    encoding="auto",                 # DÃ©tection auto avec chardet
    delimiter=",",                   # SÃ©parateur CSV
    meta_columns=[],                 # Si vide : toutes sauf text_column
    skip_empty=True,                 # Ignorer lignes avec texte vide
    add_row_index=True,              # Ajouter row_index dans meta
    source_type="csv"                # Type de source pour Document
)
```

#### `ingest_csv()` (ligne 117-217)

Fonction principale d'ingestion :

```python
def ingest_csv(
    csv_path: Union[str, Path],
    config: Optional[CSVIngestionConfig] = None
) -> List[Document]:
    """
    IngÃ¨re un fichier CSV et retourne une liste de Documents.

    Args:
        csv_path: Chemin vers le fichier CSV
        config: Configuration (utilise dÃ©faut si None)

    Returns:
        Liste de Documents (core.document.Document)

    Raises:
        CSVIngestionError: Erreur lors de l'ingestion
    """
```

### Flux d'ingestion

```python
# Ligne 131-161 : DÃ©tection d'encodage
if config.encoding == "auto":
    with open(csv_path, 'rb') as f:
        result = chardet.detect(f.read(100000))
        encoding = result['encoding']
else:
    encoding = config.encoding

# Ligne 163-172 : Lecture CSV avec pandas
df = pd.read_csv(csv_path, encoding=encoding, delimiter=config.delimiter)

# Ligne 174-182 : Validation colonne texte
if config.text_column not in df.columns:
    raise CSVIngestionError(f"Colonne '{config.text_column}' introuvable")

# Ligne 184-217 : CrÃ©ation des Documents
documents = []
for idx, row in df.iterrows():
    texte = str(row[config.text_column]).strip()

    if config.skip_empty and not texte:
        continue

    # Construction mÃ©tadonnÃ©es (toutes colonnes sauf text_column)
    meta = {}
    for col in df.columns:
        if col != config.text_column:
            meta[col] = row[col]

    if config.add_row_index:
        meta["row_index"] = idx

    meta["source_type"] = config.source_type

    # CrÃ©ation Document
    doc = Document(texteocr=texte, meta=meta)
    documents.append(doc)
```

### Exemple d'utilisation complÃ¨te

```python
from ingestion.csv_ingestion import ingest_csv, CSVIngestionConfig

# Configuration personnalisÃ©e
config = CSVIngestionConfig(
    text_column="article_text",
    encoding="utf-8",
    skip_empty=True
)

# Ingestion
documents = ingest_csv("data/articles.csv", config)

# Conversion pour rad_chunk.py
import pandas as pd
df = pd.DataFrame([doc.to_dict() for doc in documents])
df.to_csv("output.csv", index=False)

# Le CSV output.csv contient maintenant :
# - texteocr : Contenu de la colonne article_text
# - Toutes les autres colonnes du CSV original
# - source_type : "csv"
# - texteocr_provider : absent (pas de recodage GPT)
```

### Gestion des erreurs

```python
# Ligne 31-33 : Exception personnalisÃ©e
class CSVIngestionError(Exception):
    """Exception levÃ©e lors d'erreurs d'ingestion CSV."""
    pass

# Erreurs courantes gÃ©rÃ©es :
# - Colonne texte introuvable
# - Fichier CSV corrompu
# - Encodage invalide
# - Toutes les lignes vides aprÃ¨s filtrage
```

### IntÃ©gration avec le pipeline

L'ingestion CSV s'intÃ¨gre **directement aprÃ¨s l'Ã©tape 1** du pipeline :

```text
CSV source â†’ csv_ingestion.py â†’ List[Document] â†’ to_dict() â†’ output.csv
                                                                  â†“
                                                            rad_chunk.py
```

Avantage : **Pas de recodage GPT** car `texteocr_provider` absent (Ã©conomie API).

---

## 1C. Module `rad_dataframe.py` â€” Extraction PDF/OCR

### ResponsabilitÃ©s

- Charger un export Zotero (JSON)
- Localiser les PDF rÃ©fÃ©rencÃ©s (avec recherche fuzzy si nÃ©cessaire)
- Extraire le texte via OCR multi-provider
- Produire un CSV avec mÃ©tadonnÃ©es + texte

### Fonctions clÃ©s

| Fonction | Localisation | RÃ´le |
|----------|-------------|------|
| `extract_text_with_ocr()` | ligne 337 | Point d'entrÃ©e OCR avec cascade Mistral â†’ OpenAI â†’ Legacy |
| `_extract_text_with_mistral()` | ligne 144 | OCR Mistral (retourne Markdown) |
| `_extract_text_with_openai()` | ligne 267 | OCR OpenAI vision (fallback) |
| `_extract_text_with_legacy_pdf()` | ligne 120 | OCR PyMuPDF (fallback final) |
| `load_zotero_to_dataframe()` | ligne 397 | Orchestration : JSON â†’ DataFrame |

### Flux dÃ©taillÃ©

```python
# Ligne 490-509 : Extraction OCR avec dÃ©tails
ocr_payload = extract_text_with_ocr(
    actual_pdf_path,
    return_details=True,  # Retourne OCRResult(text, provider)
)

# Ligne 503-510 : Construction du record
records.append({
    # MÃ©tadonnÃ©es Zotero
    "type": ...,
    "title": ...,
    "authors": ...,
    "date": ...,
    "url": ...,
    "doi": ...,
    "filename": ...,
    "path": ...,
    "attachment_title": ...,

    # Texte OCR - POINT CRITIQUE
    "texteocr": ocr_payload.text,           # â† Source unique du texte
    "texteocr_provider": ocr_payload.provider  # â† "mistral" | "openai" | "legacy"
})
```

### Colonnes CSV produites (hardcodÃ©es)

| Colonne | Type | Description |
|---------|------|-------------|
| `type` | str | Type d'item Zotero ("article", "book", etc.) |
| `title` | str | Titre du document |
| `authors` | str | Auteurs (jointure par virgule) |
| `date` | str | Date de publication |
| `url` | str | URL de l'article |
| `doi` | str | Digital Object Identifier |
| `filename` | str | Nom du fichier PDF |
| `path` | str | Chemin complet rÃ©solu du PDF |
| `attachment_title` | str | Titre de l'attachement Zotero |
| **`texteocr`** | **str** | **Texte extrait par OCR (variable pivot)** |
| **`texteocr_provider`** | **str** | **Fournisseur OCR utilisÃ©** |

### Variables d'environnement

```bash
# OCR Mistral (prioritaire)
MISTRAL_API_KEY
MISTRAL_API_BASE_URL="https://api.mistral.ai"
MISTRAL_OCR_MODEL="mistral-ocr-latest"
MISTRAL_OCR_TIMEOUT=300
MISTRAL_DELETE_UPLOADED_FILE=true

# OCR OpenAI (fallback)
OPENAI_API_KEY
OPENAI_OCR_MODEL="gpt-4o-mini"
OPENAI_OCR_PROMPT="Transcris cette page PDF en Markdown..."
OPENAI_OCR_MAX_PAGES=10
OPENAI_OCR_MAX_TOKENS=2048
OPENAI_OCR_RENDER_SCALE=2.0
```

---

## 2. Module `rad_chunk.py` â€” Chunking et embeddings

### ResponsabilitÃ©s

- DÃ©couper `texteocr` en chunks (~1000 tokens)
- Recoder les chunks via GPT/OpenRouter (sauf si OCR Mistral ou CSV)
- GÃ©nÃ©rer embeddings denses (OpenAI `text-embedding-3-large`)
- GÃ©nÃ©rer embeddings sparses (spaCy TF lemmatisÃ©)

### NOUVEAU (2025-01-25) : Support OpenRouter

Le script supporte maintenant **OpenRouter** comme alternative Ã©conomique Ã  OpenAI pour le recodage de texte :

```bash
# Utiliser OpenAI (dÃ©faut)
python rad_chunk.py --input data.csv --output ./out --phase initial

# Utiliser OpenRouter (2-3x moins cher)
python rad_chunk.py --input data.csv --output ./out --phase initial \
  --model openai/gemini-2.5-flash
```

#### Auto-dÃ©tection du provider (ligne 121-140)

```python
def gpt_recode_batch(chunks, instructions, model="gpt-4o-mini", ...):
    # Auto-dÃ©tection basÃ©e sur le format du modÃ¨le
    use_openrouter = "/" in model  # "provider/model" = OpenRouter
    active_client = openrouter_client if use_openrouter else client

    if use_openrouter and not openrouter_client:
        print(f"Warning: OpenRouter model '{model}' requested but unavailable.")
        print("Falling back to OpenAI gpt-4o-mini")
        model = "gpt-4o-mini"
        active_client = client

    print(f"Using {'OpenRouter' if use_openrouter else 'OpenAI'} with model: {model}")
```

**Logique de fallback** : Si OpenRouter indisponible â†’ bascule automatiquement vers OpenAI.

#### Client OpenRouter (ligne 72-82)

```python
# OpenRouter Client Initialization (optional)
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
openrouter_client = None
if OPENROUTER_API_KEY:
    openrouter_client = OpenAI(
        api_key=OPENROUTER_API_KEY,
        base_url="https://openrouter.ai/api/v1"
    )
    print("OpenRouter client initialized successfully.")
else:
    print("OpenRouter API key not found. Will use OpenAI for all LLM calls.")
```

### Fonctions clÃ©s

| Fonction | Localisation | RÃ´le |
|----------|-------------|------|
| `process_document_chunks()` | ligne 187 | Orchestration chunking + recodage pour 1 document |
| `gpt_recode_batch()` | ligne 109 | Recodage GPT par lot (5 chunks par dÃ©faut) |
| `get_embeddings_batch()` | ligne 301 | Embeddings denses OpenAI |
| `extract_sparse_features()` | ligne 409 | Embeddings sparses spaCy |
| `save_raw_chunks_to_json_incrementally()` | ligne 169 | Sauvegarde thread-safe incrÃ©mentale |

### Flux de traitement (3 phases)

#### Phase `initial` : CSV â†’ Chunks JSON

```python
# Ligne 199 : POINT D'ENTRÃ‰E du texte
text = row_data.get("texteocr", "").strip()

# Ligne 232-237 : DÃ©cision de recodage selon provider (MISE Ã€ JOUR 2025-01-25)
provider = str(row_data.get("texteocr_provider", "")).lower()
recode_required = provider not in ("mistral", "csv")  # Skip si Mistral OU CSV

# Ligne 211 : Chunking
text_chunks = TEXT_SPLITTER.split_text(text)  # RecursiveCharacterTextSplitter

# Ligne 221-233 : Recodage conditionnel
if recode_required:
    cleaned_batch = gpt_recode_batch(
        batch_to_recode,
        instructions="Nettoie ce chunk OCR brut...",
        model="gpt-4o-mini"
    )
else:
    cleaned_batch = batch_to_recode  # Pas de recodage
```

#### MÃ©tadonnÃ©es des chunks (hardcodÃ©es !)

```python
# Ligne 250-263 : Construction de chunk_metadata
chunk_metadata = {
    "id":           f"{doc_id}_{original_chunk_index}",
    "type":         sanitize_metadata_value(row_data.get("type", "")),
    "title":        sanitize_metadata_value(row_data.get("title", "")),
    "authors":      sanitize_metadata_value(row_data.get("authors", "")),
    "date":         sanitize_metadata_value(row_data.get("date", "")),
    "filename":     sanitize_metadata_value(row_data.get("filename", "")),
    "doc_id":       doc_id,
    "chunk_index":  original_chunk_index,
    "total_chunks": len(text_chunks),
    "text":         cleaned_text,
    "ocr_provider": sanitize_metadata_value(provider, ""),
}
```

**ProblÃ¨me identifiÃ©** : Liste de champs **hardcodÃ©e** ! Toute colonne CSV non listÃ©e ici est **perdue**.

#### Phase `dense` : Chunks â†’ Chunks + Embeddings denses

```python
# Ligne 301-318 : Embeddings denses OpenAI
def get_embeddings_batch(texts, model="text-embedding-3-large"):
    response = client.embeddings.create(input=texts, model=model)
    return [item.embedding for item in response.data]
```

- Traite les chunks par lots de 32 (configurable)
- Retries automatiques en cas d'erreur API
- Sauvegarde intermÃ©diaire tous les ~1000 chunks

#### Phase `sparse` : Chunks â†’ Chunks + Embeddings sparses

```python
# Ligne 409-449 : Embeddings sparses spaCy
def extract_sparse_features(text):
    doc = nlp(text)  # spaCy fr_core_news_md
    relevant_pos = {"NOUN", "PROPN", "ADJ", "VERB"}
    lemmas = [token.lemma_.lower() for token in doc
              if token.pos_ in relevant_pos
              and not token.is_stop
              and len(token.lemma_) > 1]

    # TF normalisÃ© avec hachage mod 100k
    counts = Counter(lemmas)
    for lemma, count in counts.items():
        index = hash(lemma) % 100000
        sparse_dict[str(index)] = count / total_lemmas_in_doc

    return {"indices": [...], "values": [...]}
```

### Variables d'environnement

```bash
OPENAI_API_KEY  # Obligatoire pour embeddings + recodage
```

### Configurations internes

```python
DEFAULT_MAX_WORKERS = os.cpu_count() - 1  # ParallÃ©lisme gÃ©nÃ©ral
DEFAULT_BATCH_SIZE_GPT = 5                # Recodage GPT
DEFAULT_EMBEDDING_BATCH_SIZE = 32         # Embeddings denses
TEXT_SPLITTER:
  - chunk_size: 1000 tokens
  - chunk_overlap: 150 tokens
  - separators: ["\n\n", "#", "##", "\n", " ", ""]
```

---

## 3. Module `rad_vectordb.py` â€” Insertion vectorielle

### ResponsabilitÃ©s
- Charger JSON avec embeddings (denses + sparses)
- Formatter les donnÃ©es pour chaque provider
- Upserter par lots vers Pinecone / Weaviate / Qdrant

### Fonctions clÃ©s par provider

#### Pinecone

| Fonction | Localisation | RÃ´le |
|----------|-------------|------|
| `prepare_vectors_for_pinecone()` | ligne 66 | Formatte chunks â†’ vecteurs Pinecone |
| `upsert_batch_to_pinecone()` | ligne 29 | Upsert 1 lot avec retry |
| `insert_to_pinecone()` | ligne 123 | Orchestration complÃ¨te |

```python
# Ligne 85-95 : MÃ‰TADONNÃ‰ES HARDCODÃ‰ES
metadata = {
    "title": chunk.get("title", ""),
    "authors": chunk.get("authors", ""),
    "date": chunk.get("date", ""),
    "type": chunk.get("type", ""),
    "filename": chunk.get("filename", ""),
    "doc_id": chunk.get("doc_id", ""),
    "chunk_index": chunk.get("chunk_index", 0),
    "total_chunks": chunk.get("total_chunks", 0),
    "text": chunk.get("text") or chunk.get("chunk_text", "")
}
```

**ProblÃ¨me** : MÃ©tadonnÃ©es **hardcodÃ©es** ! Impossible d'injecter des colonnes CSV arbitraires.

#### Weaviate

| Fonction | Localisation | RÃ´le |
|----------|-------------|------|
| `insert_to_weaviate_hybrid()` | ligne 436 | Orchestration complÃ¨te avec multi-tenancy |
| `generate_uuid()` | ligne 385 | UUID v5 stable |
| `normalize_date_to_rfc3339()` | ligne 399 | Conversion dates â†’ RFC3339 |

```python
# Ligne 541-551 : MÃ‰TADONNÃ‰ES HARDCODÃ‰ES (mÃªme liste)
properties = {
    "title": chunk.get("title", ""),
    "authors": chunk.get("authors", ""),
    "date": normalize_date_to_rfc3339(chunk.get("date", "")),
    "type": chunk.get("type", ""),
    "filename": chunk.get("filename", ""),
    "doc_id": chunk.get("doc_id", ""),
    "chunk_index": chunk.get("chunk_index", 0),
    "total_chunks": chunk.get("total_chunks", 0),
    "text": chunk.get("text") or chunk.get("chunk_text", "")
}
```

#### Qdrant

| Fonction | Localisation | RÃ´le |
|----------|-------------|------|
| `prepare_points_for_qdrant()` | ligne 604 | Formatte chunks â†’ PointStruct |
| `upsert_batch_to_qdrant()` | ligne 661 | Upsert 1 lot avec retry |
| `insert_to_qdrant()` | ligne 709 | Orchestration complÃ¨te |

```python
# Ligne 636-647 : MÃ‰TADONNÃ‰ES HARDCODÃ‰ES (mÃªme liste)
payload = {
    "original_id": chunk["id"],
    "title": chunk.get("title", ""),
    "authors": chunk.get("authors", ""),
    "date": chunk.get("date", ""),
    "type": chunk.get("type", ""),
    "filename": chunk.get("filename", ""),
    "doc_id": chunk.get("doc_id", ""),
    "chunk_index": chunk.get("chunk_index", 0),
    "total_chunks": chunk.get("total_chunks", 0),
    "text": chunk.get("text") or chunk.get("chunk_text", "")
}
```

### Configurations

```python
PINECONE_BATCH_SIZE = 100
WEAVIATE_BATCH_SIZE = 100
QDRANT_BATCH_SIZE = 100
```

### Variables d'environnement

```bash
# Pinecone
PINECONE_API_KEY
PINECONE_ENV  # Optionnel selon SDK version

# Weaviate
WEAVIATE_URL
WEAVIATE_API_KEY

# Qdrant
QDRANT_URL
QDRANT_API_KEY  # Optionnel (instances locales)
```

---

## 4. Utilitaire `scripts/crawl.py` â€” Crawling web vers PDF/Markdown

### ResponsabilitÃ©s

Script autonome pour crawler des sites web et convertir les pages en PDF ou Markdown. Utile pour crÃ©er des sources documentaires Ã  partir de documentation en ligne.

### FonctionnalitÃ©s

```python
# Configuration (ligne 20-23)
START_URL = "https://docs.n8n.io/integrations/"
DOMAIN = urlparse(START_URL).netloc
PDF_DIR = "pages_pdf"
MD_DIR = "pages_md"
```

#### Crawling rÃ©cursif (ligne 64-103)

```python
def crawl(url):
    if url in VISITED:
        return
    VISITED.add(url)

    # TÃ©lÃ©chargement HTML
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")

    # Conversion PDF (pdfkit ou Playwright en fallback)
    save_pdf(url, filename_base)

    # Conversion Markdown (optionnel)
    save_markdown(url, filename_base, soup)

    # Exploration liens internes
    for link in soup.find_all("a", href=True):
        abs_url = urljoin(url, link["href"])
        if is_internal_link(abs_url):
            crawl(abs_url)  # RÃ©cursif
```

#### Conversion PDF (ligne 41-62)

Deux mÃ©thodes avec fallback automatique :

1. **pdfkit** (prioritaire) : Utilise `wkhtmltopdf` si disponible
2. **Playwright** (fallback) : Ã‰mulation navigateur headless

```python
def save_pdf(url, filename_base):
    if PDFKIT_AVAILABLE:
        try:
            pdfkit.from_url(url, filepath, configuration=config_pdfkit)
            return
        except Exception as e:
            print(f"âš ï¸ Erreur pdfkit, tentative Playwright : {e}")

    # Fallback Playwright
    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        page.pdf(path=filepath)
        browser.close()
```

### Cas d'usage

Documenter des sites entiers pour ingestion RAG :

```bash
# 1. Crawler et tÃ©lÃ©charger les PDFs
python scripts/crawl.py  # Modifie START_URL dans le script

# 2. IngÃ©rer les PDFs dans le pipeline
# (Via rad_dataframe.py ou directement via app/main.py)
```

### DÃ©pendances

```bash
pip install requests beautifulsoup4 playwright pdfkit
python -m playwright install chromium
# Optionnel : installer wkhtmltopdf pour pdfkit
```

**Note** : Cet utilitaire est **indÃ©pendant** du pipeline principal. Il crÃ©e des PDFs qui peuvent ensuite Ãªtre traitÃ©s par `rad_dataframe.py`.

---

## Points critiques identifiÃ©s pour l'ingestion CSV

### 1. Variable pivot unique : `texteocr`

| Point de crÃ©ation/consommation | Fichier | Ligne |
|-------------------------------|---------|-------|
| **CrÃ©ation (OCR)** | rad_dataframe.py | 508 |
| **Consommation (chunking)** | rad_chunk.py | 199 |

**Conclusion** : `texteocr` est le **seul point d'entrÃ©e** du contenu textuel dans le pipeline. Pour ingÃ©rer du CSV, il suffit de mapper une colonne CSV â†’ `texteocr`.

### 2. MÃ©tadonnÃ©es hardcodÃ©es (3 emplacements)

| Emplacement | Fichier | Ligne | Impact |
|------------|---------|-------|--------|
| CrÃ©ation chunks | rad_chunk.py | 250-263 | Colonnes CSV non listÃ©es â†’ perdues |
| PrÃ©paration Pinecone | rad_vectordb.py | 85-95 | Impossible d'ajouter champs CSV |
| PrÃ©paration Weaviate | rad_vectordb.py | 541-551 | Impossible d'ajouter champs CSV |
| PrÃ©paration Qdrant | rad_vectordb.py | 636-647 | Impossible d'ajouter champs CSV |

**Conclusion** : Les 3 connecteurs utilisent la **mÃªme liste hardcodÃ©e** de champs. Refactorisation nÃ©cessaire pour accepter des mÃ©tadonnÃ©es dynamiques.

### 3. Logique de recodage GPT liÃ©e Ã  `texteocr_provider` âœ… **IMPLÃ‰MENTÃ‰ 2025-01-25**

```python
# rad_chunk.py:232-237 (MISE Ã€ JOUR)
provider = str(row_data.get("texteocr_provider", "")).lower()
recode_required = provider not in ("mistral", "csv")  # âœ… CSV supportÃ© !
```

**Statut** : âœ… **RÃ‰SOLU** - L'ingestion CSV ajoute automatiquement `source_type="csv"` et le script skip le recodage GPT, Ã©conomisant des coÃ»ts API.

### 4. Fonction `sanitize_metadata_value()` (ligne 242-248)

GÃ¨re les types incompatibles JSON/Pinecone :
- Convertit `pd.NA` / `np.nan` â†’ chaÃ®ne vide
- Assure types primitifs : str, int, float, bool
- Fallback vers `str(value)`

**Bon point** : DÃ©jÃ  robuste pour gÃ©rer des colonnes CSV hÃ©tÃ©rogÃ¨nes !

---

## DÃ©pendances du pipeline

### DÃ©pendances Python critiques

```txt
# Chunking & NLP
langchain-text-splitters  # RecursiveCharacterTextSplitter
spacy==3.x + fr_core_news_md

# Embeddings & OCR
openai>=1.x
mistralai
requests

# Manipulation donnÃ©es
pandas
tqdm
python-dotenv

# Bases vectorielles
pinecone>=3.x          # SDK v3+ avec Pinecone class
weaviate-client>=4.x   # Multi-tenancy support
qdrant-client

# Utilitaires
python-dateutil        # Pour Weaviate RFC3339
fitz (PyMuPDF)         # Fallback OCR legacy
```

### ModÃ¨les externes

| Service | ModÃ¨le par dÃ©faut | Usage |
|---------|------------------|-------|
| Mistral OCR | `mistral-ocr-latest` | Extraction PDF â†’ Markdown |
| OpenAI vision | `gpt-4o-mini` | Fallback OCR |
| OpenAI recodage | `gpt-4o-mini` | Nettoyage chunks OCR bruts |
| OpenAI embeddings | `text-embedding-3-large` | Embeddings denses (3072 dim) |
| spaCy | `fr_core_news_md` | Lemmatisation + POS tagging |

---

## Ã‰tat de la refactorisation et opportunitÃ©s futures

### âœ… OpportunitÃ© 1 : Abstraction de la source de `texteocr` â€” **IMPLÃ‰MENTÃ‰E**

**Ã‰tat actuel** : âœ… **RÃ‰ALISÃ‰** via `core/document.py` et `ingestion/csv_ingestion.py`

```python
# core/document.py - Abstraction unifiÃ©e
@dataclass
class Document:
    texteocr: str
    meta: Dict[str, Any]

# ingestion/csv_ingestion.py - ImplÃ©mentation CSV
def ingest_csv(csv_path, config) -> List[Document]:
    # Lecture CSV â†’ mapping colonne â†’ Document
    pass

# Workflow complet
documents = ingest_csv("data.csv", config)
df = pd.DataFrame([doc.to_dict() for doc in documents])
df.to_csv("output.csv")  # Compatible rad_chunk.py
```

**Statut** : âœ… **ImplÃ©mentÃ©** - La classe `Document` et l'ingestion CSV sont opÃ©rationnelles depuis le dÃ©veloppement rÃ©cent.

### OpportunitÃ© 2 : MÃ©tadonnÃ©es dynamiques

**Ã‰tat actuel** : Liste de 10 champs hardcodÃ©e dans 4 emplacements.

**Cible** : Remplacer par une **injection complÃ¨te de `meta`** :

```python
# rad_chunk.py - AVANT
chunk_metadata = {
    "title": row_data.get("title", ""),
    "authors": row_data.get("authors", ""),
    # ... 8 autres champs
}

# rad_chunk.py - APRÃˆS
chunk_metadata = {
    "id": ...,
    "text": ...,
    **row_data.get("meta", {})  # Injection de toutes les mÃ©tadonnÃ©es
}
```

```python
# rad_vectordb.py - AVANT
metadata = {
    "title": chunk.get("title", ""),
    # ... liste hardcodÃ©e
}

# rad_vectordb.py - APRÃˆS
metadata = {k: v for k, v in chunk.items()
            if k not in ("id", "embedding", "sparse_embedding", "text")}
# Ou : metadata = chunk.get("meta", {})
```

### OpportunitÃ© 3 : Configuration CSV flexible

```yaml
# config/csv_config.yaml
csv:
  text_column: "text"        # Colonne source de texteocr
  encoding: "auto"            # utf-8, latin-1, auto-detect
  delimiter: ","
  meta_columns: []            # Si vide : toutes sauf text_column
  skip_empty: true            # Ignorer lignes avec texte vide
  add_metadata:
    source_type: "csv"
    ingested_at: "{{timestamp}}"
```

### âœ… OpportunitÃ© 4 : Classe `Document` unifiÃ©e â€” **IMPLÃ‰MENTÃ‰E**

**Ã‰tat actuel** : âœ… **RÃ‰ALISÃ‰** dans `core/document.py`

```python
@dataclass
class Document:
    texteocr: str
    meta: Dict[str, Any]

    def __post_init__(self):
        if not self.texteocr or not self.texteocr.strip():
            raise ValueError("texteocr ne peut pas Ãªtre vide")
        if "source_type" not in self.meta:
            self.meta["source_type"] = "unknown"

    def to_dict(self) -> Dict[str, Any]:
        return {"texteocr": self.texteocr, **self.meta}
```

**Statut** : âœ… **ImplÃ©mentÃ©** - Toutes les sources d'ingestion utilisent cette classe.

---

## Risques identifiÃ©s

| Risque | ProbabilitÃ© | Mitigation |
|--------|-------------|------------|
| MÃ©tadonnÃ©es CSV trop volumineuses pour Pinecone | Moyenne | Filtrer/tronquer les valeurs longues + warning |
| RÃ©gression sur pipeline PDF | Faible | Suite de tests de non-rÃ©gression |
| CSV mal encodÃ©s | Ã‰levÃ©e | DÃ©tection auto (chardet) + fallback UTF-8 |
| Colonnes CSV avec noms invalides (espaces, spÃ©ciaux) | Moyenne | Sanitisation via `re.sub(r'[^a-zA-Z0-9_]', '_', col)` |
| Recodage GPT activÃ© par erreur sur CSV | Moyenne | Ajout de `texteocr_provider="csv"` automatique |

---

## Prochaines Ã©tapes recommandÃ©es

### Phase 1 : AmÃ©lioration de l'intÃ©gration CSV âœ… **PARTIELLEMENT RÃ‰ALISÃ‰E**

1. âœ… **Module `csv_ingestion.py`** â€” IMPLÃ‰MENTÃ‰
   - âœ… Fonction `ingest_csv()` â†’ `List[Document]`
   - âœ… Configuration du mapping colonnes via `CSVIngestionConfig`
   - âœ… Validations et logging

2. âœ… **Support CSV dans `rad_chunk.py`** â€” IMPLÃ‰MENTÃ‰
   - âœ… Condition `provider in ("mistral", "csv")` pour skip recodage
   - âœ… Support OpenRouter pour rÃ©duction des coÃ»ts (2025-01-25)

3. âš ï¸ **Refactorisation de `rad_vectordb.py`** â€” EN ATTENTE
   - âŒ MÃ©tadonnÃ©es toujours hardcodÃ©es dans les 3 connecteurs
   - âŒ Injection dynamique de `meta` non implÃ©mentÃ©e
   - Impact : Les mÃ©tadonnÃ©es CSV personnalisÃ©es ne sont pas insÃ©rÃ©es dans les bases vectorielles

### Phase 2 : Tests et validation

1. âš ï¸ **Tests de bout en bout** â€” PARTIELLEMENT TESTÃ‰S
   - âœ… Ingestion CSV â†’ chunks â†’ embeddings fonctionne
   - âŒ VÃ©rification complÃ¨te de l'injection des mÃ©tadonnÃ©es CSV dans Pinecone/Weaviate/Qdrant
   - âŒ Tests de recherche avec filtres sur mÃ©tadonnÃ©es CSV personnalisÃ©es

### Phase 3 : AmÃ©liorations futures

1. **Interface web pour CSV direct** â€” EN COURS
   - âš ï¸ Endpoint `/upload_csv_direct` existe dans app/main.py mais nÃ©cessite intÃ©gration UI complÃ¨te

2. **Configuration flexible** â€” Ã€ DÃ‰VELOPPER
   - CrÃ©er un systÃ¨me de configuration YAML/JSON pour le mapping CSV
   - Permettre la configuration via l'interface web

---

## Conclusion

**Ã‰tat actuel (2025-01-25)** :

Le pipeline RAGpy a considÃ©rablement Ã©voluÃ© depuis sa documentation initiale :

### âœ… RÃ©alisations majeures

1. **Orchestration web complÃ¨te** via `app/main.py` (1042 lignes)
   - Interface utilisateur intuitive
   - Gestion de sessions et uploads
   - Configuration credentials via UI

2. **Ingestion multi-sources** :
   - âœ… PDF/OCR (Mistral â†’ OpenAI â†’ PyMuPDF)
   - âœ… CSV direct (`ingestion/csv_ingestion.py`)
   - âœ… Classe `Document` unifiÃ©e (`core/document.py`)

3. **Optimisation des coÃ»ts** :
   - âœ… Support OpenRouter (Ã©conomie ~75% sur recodage)
   - âœ… Skip recodage automatique pour CSV et Mistral OCR

4. **Robustesse** :
   - âœ… Logging centralisÃ©
   - âœ… Gestion d'erreurs subprocess
   - âœ… Timeouts configurables

### âš ï¸ Points d'attention

1. **MÃ©tadonnÃ©es vectorielles** : Les 3 connecteurs (Pinecone/Weaviate/Qdrant) ont toujours des mÃ©tadonnÃ©es hardcodÃ©es. Les colonnes CSV personnalisÃ©es ne sont pas injectÃ©es dans les bases vectorielles.

2. **Tests E2E** : L'injection complÃ¨te CSV â†’ base vectorielle avec mÃ©tadonnÃ©es personnalisÃ©es n'a pas Ã©tÃ© validÃ©e.

### ğŸ¯ Recommandation prioritaire

**Refactoriser `rad_vectordb.py`** pour accepter des mÃ©tadonnÃ©es dynamiques :

```python
# Au lieu de :
metadata = {"title": chunk.get("title"), "authors": ..., ...}

# Utiliser :
metadata = {k: v for k, v in chunk.items()
            if k not in ("id", "embedding", "sparse_embedding", "text")}
```

Cela permettra aux mÃ©tadonnÃ©es CSV de se propager jusqu'aux bases vectorielles, dÃ©bloquant les cas d'usage de filtrage avancÃ©.
